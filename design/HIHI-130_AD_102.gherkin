Feature: Encrypt PII Data in Customer 360 Table

  Background:
    Given the table "purgo_playground.customer_360_raw_clone" does not exist
    When the table "purgo_playground.customer_360_raw_clone" is created as a replica of "purgo_playground.customer_360_raw"
    Then the table "purgo_playground.customer_360_raw_clone" is ready for encryption

  Scenario: Encrypt PII columns in the customer_360_raw_clone table
    Given the PII columns "name", "email", "phone", and "zip" in the table "purgo_playground.customer_360_raw_clone"
    When the encryption method "AES-256" is applied to these columns
    Then the data in these columns is encrypted
    And the encryption key is saved as a JSON file named "encryption_key_<current_datetime>.json" in the location "/Volumes/agilisium_playground/purgo_playground/de_dq"

  Scenario: Validate encrypted data
    Given the PII columns "name", "email", "phone", and "zip" are encrypted
    When the data is validated for encryption
    Then the data should not be readable in its original form
    And the data should be decryptable using the saved encryption key

  Scenario: Error handling during encryption
    Given the encryption process is initiated
    When an error occurs during encryption
    Then the process should log the error with a message "Encryption failed for column <column_name>"
    And the process should rollback any partial changes

  Scenario Outline: Data-driven encryption validation
    Given the PII column "<column_name>" in the table "purgo_playground.customer_360_raw_clone"
    When the encryption method "AES-256" is applied
    Then the data in the column "<column_name>" is encrypted
    And the encrypted data is validated for correctness

    Examples:
      | column_name |
      | name        |
      | email       |
      | phone       |
      | zip         |

  Scenario: Performance benchmark for encryption process
    Given the encryption process is initiated
    When the process is completed
    Then the total time taken should be less than "5 minutes"
    And the process should handle up to "1 million" records efficiently